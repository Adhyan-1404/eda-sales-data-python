# EDA on Sales Data 

_Analyzing sales data to deliver clean, actionable insights and prepare for predictive modeling using Python and Jupyter Notebook._
<a id="top"></a>

---

## Table of Contents
- <a href="#overview">Overview</a>
- <a href="#objectives">Objectives</a>
- <a href="#tools--technologies">Tools & Technologies</a>
- <a href="#key-steps-performed">Key Steps Performed</a>
- <a href="#repository-files">Repository Files</a>
- <a href="#notes">Notes</a>
- <a href="#how-to-run-this-project">How to Run This Project</a>
- <a href="#final-recommendations">Final Recommendations</a>
- <a href="#Author">Author</a>

---

<h2><a class="anchor" id="overview"></a>Overview</h2>

This project performs Exploratory Data Analysis (EDA) focused on cleaning and preprocessing a raw sales dataset (`k_circle_sales.csv`) sourced from Kaggle. 
The primary goal is to transform raw, messy sales data into a consistent, structured, and machine-learning-ready format. 
This preparation ensures high data quality for downstream predictive modeling and analysis.

---

<h2><a class="anchor" id="objectives"></a>Objectives</h2>

- Clean and preprocess raw sales data to ensure accuracy and consistency.
- Perform exploratory data analysis to identify patterns, trends, and anomalies.
- Engineer features relevant for future predictive modeling.
- Detect and treat outliers to improve data quality.
- Prepare the dataset to be machine-learning ready for advanced analysis.

---

<h2><a class="anchor" id="tools--technologies"></a>Tools & Technologies</h2>

1. Python (core language) with key libraries :
   - NumPy (numerical computing)
   - Pandas (data manipulation)
   - Matplotlib and Seaborn (visualization)
   - Warnings module (to manage runtime warnings)

2. IDEs :
   - Jupyter Notebook
   - VScode

---

<h2><a class="anchor" id="key-steps-performed"></a>Key Steps Performed</h2>

- Handling and imputing **Missing Values** to ensure dataset completeness.
- **Feature Engineering** to create meaningful variables for machine learning models
- **Detecting** and **Treating Outliers** to improve data robustness
- **Correcting incorrect** or **Inconsistent Values** (e.g., negative sales, invalid dates)
- Generating **Summary Statistics** for data validation
- Reducing **Skewness** and applying **Transformations** to approximate **Normality**, optimizing model performance

These steps were accomplished using a combination of data transformations, imputation techniques, and visualizations to get a comprehensive understanding of the dataset.

---

<h2><a class="anchor" id="repository-files"></a>Repository Files</h2>

* `eda_sales_data.ipynb` â€“ Jupyter Notebook containing the full code and detailed workflow of the data cleaning and EDA process.
* `eda_sales_data.html` â€“ Rendered HTML version of the notebook showcasing code and output (non-editable).
* `k_circle_sales.csv` - Raw sales data sourced from Kaggle.
* `README.md` - This project overview.

---

<h2><a class="anchor" id="notes"></a>Notes</h2>

- Data cleaning and preprocessing were performed entirely in Python using related libraries.
- Visualizations and insights aim to support strategic decision-making and predictive model development.
- The notebook environment (Jupyter/Colab) facilitates interactive exploration and reproducibility.

---

<h2><a class="anchor" id="how-to-run-this-project"></a>How to Run This Project</h2>

Make sure to update the **file path** for `k_circle_sales.csv` in the notebook **EDA on Sales Data.ipynb** to match your local directory. Failing to do so will result in a **error**. Also make sure all the necessary **Python libraries** mentioned in [Tools & Technologies](#tools--technologies) are already installed.

You can run the notebook directly in Jupyter or use an online service like [Google Colab](https://colab.research.google.com) by uploading the notebook file.

* Interactive Notebook : [Click to Open](./eda_sales_data.ipynb)
* Static HTML : [Click to Open](./eda_sales_data.html)

---

<h2><a class="anchor" id="final-recommendations"></a>Final Recommendations</h2>

- Use the clean and validated dataset as the foundation for future predictive modeling projects.
- Continuously update and validate data preprocessing as new data becomes available.
- Plan for advanced modeling by addressing highlighted data quality or feature engineering gaps.

---

<h2><a class="anchor" id="author"></a>Author</h2>

Adhyan Saxena    
ðŸ”— [LinkedIn](https://www.linkedin.com/in/adhyan1404)  
ðŸ”— [GitHub](https://github.com/Adhyan-1404)

---
[Return to Top](#top)


